<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Test-Time Adaptation framework for environmental monitoring.">
  <meta name="keywords" content="Test Time Adaptation, VLM, Visual Search, Visual Navigation, Ecological Monitoring">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div> -->

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://derektan95.github.io">Derek M. S. Tan</a><sup>1,4</sup>,</span>
            <span class="author-block">
              <a href="https://chinchinati.github.io/">Shailesh</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/boyang-liu-nus">Boyang Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/loki-silvres">Alok Raj</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/ang-qi-xuan-714347142">Qi Xuan Ang</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://weihengdai.top">Weiheng Dai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/tanishqduhan">Tanishq Duhan</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jimmychiun">Jimmy Chiun</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.yuhongcao.online/">Yuhong Cao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.cs.toronto.edu/~florian/">Florian Shkurti</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.marmotlab.org/bio.html">Guillaume Sartoretti</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">Under Review</span> -->
            <span class="author-block"><sup>1</sup>National University of Singapore,</span>
            <span class="author-block"><sup>2</sup>University of Toronto,</span>
            <span class="author-block"><sup>3</sup>IIT-Dhanbad,</span>
            <span class="author-block"><sup>4</sup>Singapore Technologies Engineering</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2505.11350"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.11350"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
<!--                 <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA" -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/marmotlab/Search-TTA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
<!--                 <a href="https://github.com/google/nerfies/releases/tag/0.1" -->
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" style="background-color: white;">
  <div class="container is-max-desktop" style="background-color: white;">
    <div class="hero-body" style="background-color: white;">
      <video id="teaser" autoplay muted loop playsinline height="100%"
             style="width: 100%; height: auto; display: block; background-color: white; transform: scaleX(1.002); transform-origin: left;">
        <source src="./static/videos/Real_Drone_Yosemite_1500bitrate.mp4"
                type="video/mp4">
      </video>

      <h2 class="subtitle has-text-centered">
        Visual search for bears in simulated Yosemite Valley environment (colored path = simulation path).
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
 -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            To perform autonomous visual search for environmental monitoring, a robot may leverage satellite imagery as a prior map. This can help inform coarse, high level search and exploration strategies, even when such images lack sufficient resolution to allow fine-grained, explicit visual recognition of targets. However, there are some challenges to overcome with using satellite images to direct visual search. For one, targets that are unseen in satellite images are underrepresented  in most existing datasets, and thus vision models trained on these datasets fail to reason effectively based on indirect visual cues. Furthermore, approaches which leverage large Vision Language Models (VLMs) for generalization may yield inaccurate outputs due to hallucination, leading to inefficient search. 
            <!-- (compared to real life) -->
          </p>
          <p>
            To address these challenges, we introduce <span class="dnerf">Search-TTA</span>, a multimodal test-time adaptation framework that can accept text and/or image input. First, we pretrain a remote sensing image encoder to align with CLIP’s visual encoder to output probability distributions of target presence used for visual search. Second, our framework dynamically refines CLIP’s predictions during search using a test-time adaptation mechanism. Through a feedback loop inspired by Spatial Poisson Point Processes, gradient updates (weighted by uncertainty) are used to correct (potentially inaccurate) predictions and improve search performance. To validate <span class="dnerf">Search-TTA's</span> performance, we curate a visual search dataset based on internet-scale ecological data. We find that <span class="dnerf">Search-TTA</span> <b>improves planner performance by up to 9.7%</b>, particularly in cases with poor initial CLIP predictions. It also achieves <b>comparable performance to state-of-the-art VLMs</b>. Finally, we deploy <span class="dnerf">Search-TTA</span> on a real UAV via hardware-in-the-loop testing, by simulating its operation within a large-scale simulation.
            <!-- that provides onboard sensing. -->
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Framework. -->
      <div class="column">
        <!-- <h2 class="title is-3">Framework</h2> -->
        <div class="columns is-centered">
          <div class="column is-four-fifths">
            <div class="column content">
              <!-- <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p> -->
              <video id="framework-video" controls playsinline height="100%">
                <source src="./static/videos/Search-TTA_Turtle_Example.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->


  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Approach</h2>
        <div class="content has-text-justified">
          <p>
              <span class="dnerf">Search-TTA</span> is a multimodal test-time adaptation framework that
              refines a VLM’s (potentially inaccurate) predictions online, using the agent’s measurements during
              visual search. In this work, we use CLIP as our lightweight VLM, and first align a satellite image encoder
              to the same representation space as a vision encoder through <b>patch-level contrastive learning</b>. This
              enables the satellite image encoder to generate a score map by taking the cosine similarity between
              its per-patch embeddings and the embeddings of other modalities (e.g., text, ground image). We
              then introduce a <b>novel test-time adaptation feedback mechanism</b> to refine CLIP’s predictions based
              on new measurements. To achieve this, we take inspiration from Spatial Poisson Point Processes to
              perform gradient updates to the satellite image encoder based on patches where measurements were
              taken. We also enhance the loss function with an uncertainty-driven weighting scheme to ensure
              stable gradient updates, especially at the beginning of the search process.
          </p>
          <!-- <div style="text-align: center;">
            <img src="./static/images/tta_architecture_v5.png" width="100%" />
          </div> -->
          <video id="framework-video" controls playsinline height="100%">
            <source src="./static/videos/Framework_Animation_1080p.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br />
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Dataset</h2>
        <div class="content has-text-justified">
          <p>
            To validate <span class="dnerf">Search-TTA's</span>
            performance, we curate a dataset which comprises satellite images tagged with
            the coordinates of multiple unseen taxonomic targets using internet-scale ecological data. More
            specifically, we use Sentinel-2 level 2A satellite images tagged with coordinates from the iNat-
            2021 dataset. One advantage of using ecological data is the hierarchical structure of taxonomic
            labels (seven distinct tiers), which facilitates baseline evaluation across various levels of specificity.
            In total, our dataset offers <b>437k</b> training images <b>4k</b> validation images.
          </p>
          <div style="text-align: center;">
            <img src="./static/images/visual_search_ds_examples_v4.png" width="100%" />
          </div>
        </div>
        <br />
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Examples</h2>
        <div class="content has-text-justified">
          <p>
            More coming soon...
          </p>
          <video id="framework-video" controls playsinline height="100%">
            <source src="./static/videos/SearchTTA_Marmot_Example.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br />
      </div>
    </div>
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
      <!-- <pre><code>Coming Soon!</code></pre> -->
    <pre><code>@article{tan2025searchtta,
      author    = {Derek Ming Siang Tan, Shailesh, Boyang Liu, Alok Raj, Qi Xuan Ang, Weiheng Dai, Tanishq Duhan, Jimmy Chiun, Yuhong Cao, Florian Shkurti, Guillaume Sartoretti},
      title     = {Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild},
      journal   = {Under Review},
      year      = {2025},
    }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2505.11350">
        <i class="fas fa-file-pdf"></i>
      </a>
        <a class="icon-link" href="https://github.com/marmotlab/Search-TTA" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Website template based on <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
